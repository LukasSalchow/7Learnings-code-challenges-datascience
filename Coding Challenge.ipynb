{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ppscore as pps\n",
    "from ruff_model import ruff_model, my_predictors\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Will it snow tomorrow?\" - The time traveler asked\n",
    "The following dataset contains climate information form over 9000 stations accross the world. The overall goal of t\"hese subtasks will be to predict whether it will snow tomorrow 13 years ago. So if today is 2022.02.15 then the weather we want to forecast is for the date 2009.02.16. You are suppsed to solve the tasks using Big Query, which can be used in the Jupyter Notebook like it is shown in the following cell. For further information and how to used BigQuery in Jupyter Notebook refer to the Google Docs. \n",
    "\n",
    "The goal of this test is, to test your coding knowledge in Python, BigQuery and Pandas as well as your understanding of Data Science. If you get stuck at the first part, you can use the replacement data provided in the second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"learn-360918\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery \n",
    "SELECT\n",
    "*,\n",
    "FROM `bigquery-public-data.samples.gsod`\n",
    "LIMIT 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Task\n",
    "Change the date format to 'YYYY-MM-DD' and select the data from 2005 till 2009 for station numbers including and between 725300 and 726300 , and save it as a pandas dataframe. Note the maximum year available is 2010. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery df\n",
    "SELECT\n",
    "DATE( year, month, day ) AS date,\n",
    "*,\n",
    "FROM `bigquery-public-data.samples.gsod`\n",
    "WHERE (station_number BETWEEN 725300 AND 726300)\n",
    "AND (year BETWEEN 2005 AND 2009)\n",
    "LIMIT 10 # should in principle be removed. left to avoid downloading to much stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Task \n",
    "From here want to work with the data from all stations 725300 to 725330 that have information from 2005 till 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery raw_data\n",
    "SELECT\n",
    "DATE( year, month, day ) AS date,\n",
    "*,\n",
    "FROM `bigquery-public-data.samples.gsod`\n",
    "WHERE (station_number BETWEEN 725300 AND 725330)\n",
    "AND (year BETWEEN 2005 AND 2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the stations 725300 to 725330 have information from 2005 till 2009?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.assign(date=lambda df: pd.to_datetime(df.date))\n",
    "raw_data.groupby('station_number').agg({'date':[\"min\", \"max\", 'size']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a first analysis of the remaining dataset, clean or drop data depending on how you see appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droping columns and data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(raw_data.drop(columns=['year', 'month', 'day', 'date', 'wban_number'])\n",
    " .pipe(lambda df: df.drop( columns=[col for col in df.columns if col.endswith('_samples')] ))\n",
    " .groupby('station_number').agg(lambda x:(x.isna()).mean()).astype('float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are doing a very ruff analysis we will just drop the \"_samples\" columns and all columns missing more than 10% of their values. \n",
    "Normally we should take a bit to think if there are to few \"_samples\" for something and if we can use more of the columns. \n",
    "Additionally we impute zero for missing snow_depth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = sorted(set(raw_data['station_number']))\n",
    "renumbering = dict(zip(stations,range(1,len(stations)+1)))\n",
    "\n",
    "cleaned_df = (raw_data\n",
    "              .assign(snow_depth=lambda df: df.snow_depth.fillna(0),\n",
    "                     station_number=lambda df:df.station_number.replace(renumbering))\n",
    "   [lambda df:[col for col in df.columns \n",
    "               if not col.endswith('_samples') \n",
    "                   and not df[col].isna().mean() > 0.1 \n",
    "                   and col not in {'year','month','day','wban_number','max_temperature_explicit'}]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we impute the rest of the missing values by linear interpolation over time. \n",
    "In practice we should also consider droping data, filling with values from stations near by and doing other types of interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cleaned_df.groupby('station_number').agg(lambda x:(x.isna()).mean()).astype('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.sort_values(['station_number', 'date']).pipe(lambda df: df.assign(**{col:df[col].interpolate() for col in df.columns if df[col].dtype.kind == 'f'})).sort_values(['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cleaned_df.groupby('station_number').agg(lambda x:(x.isna()).mean()).astype('float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing a first analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Weather Bureau Army Navy (WBAN) number \n",
    "\n",
    "TEMP - Mean temperature (.1 Fahrenheit)\n",
    "DEWP - Mean dew point (.1 Fahrenheit)\n",
    "SLP - Mean sea level pressure (.1 mb)\n",
    "STP - Mean station pressure (.1 mb)\n",
    "VISIB - Mean visibility (.1 miles)\n",
    "WDSP â€“ Mean wind speed (.1 knots)\n",
    "MXSPD - Maximum sustained wind speed (.1 knots)\n",
    "GUST - Maximum wind gust (.1 knots)\n",
    "MAX - Maximum temperature (.1 Fahrenheit)\n",
    "MIN - Minimum temperature (.1 Fahrenheit)\n",
    "PRCP - Precipitation amount (.01 inches)\n",
    "SNDP - Snow depth (.1 inches)        \n",
    "\n",
    "see https://www.ncei.noaa.gov/data/global-summary-of-the-day/doc/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~cleaned_df.snow).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fahrenheit_to_celsius(f):\n",
    "    return (f - 32) * 5/9\n",
    "\n",
    "def knots_to_kmph(k):\n",
    "    return 1.852*k\n",
    "\n",
    "def inches_to_cm(in_):\n",
    "    return 2.54*in_\n",
    "\n",
    "def miles_to_km(mi):\n",
    "    return 1.60934*mi\n",
    "\n",
    "def convert_units(df):\n",
    "    return df.assign(month=lambda df:df.date.dt.month,\n",
    "            mean_temp=lambda df:fahrenheit_to_celsius(df.mean_temp),\n",
    "            max_temperature=lambda df:fahrenheit_to_celsius(df.max_temperature),\n",
    "            mean_dew_point=lambda df:fahrenheit_to_celsius(df.mean_dew_point),\n",
    "            mean_visibility=lambda df:miles_to_km(df.mean_visibility),\n",
    "            mean_wind_speed=lambda df:knots_to_kmph(df.mean_wind_speed),\n",
    "            max_sustained_wind_speed=lambda df:knots_to_kmph(df.max_sustained_wind_speed),\n",
    "            total_precipitation=lambda df:inches_to_cm(df.total_precipitation),\n",
    "            snow_depth=lambda df:inches_to_cm(df.snow_depth),\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cleaned_df\n",
    " .pipe(convert_units)\n",
    " .groupby('month').mean()\n",
    " .round(3)\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data corrupted? fog,rain,snow,hail,thunder **all the same**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with known data to get a feeling for it, would also be interesting to reproduce summary data if that was avilable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.read_html('https://en.wikipedia.org/wiki/Berlin',match='Climate data for Berlin \\(SchÃ¶nefeld\\)')[0]\n",
    "    .apply(lambda x: x.str.replace(r\"\\([^\\(\\)]*\\)\",\"\"))\n",
    " .T\n",
    " .droplevel(level=0)\n",
    " .pipe(lambda df: df.rename(columns=df.iloc[0].str.replace(r'\\s*Â°C\\s*','')))\n",
    " .pipe(lambda df: df.drop(df.index[[0]]))\n",
    " .assign(month=list(range(1,13))+['year'])\n",
    " .set_index('month')\n",
    " [['Record high','Average high','Daily mean']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cleaned_df\n",
    " [lambda df: df.station_number == 1]\n",
    " .pipe(convert_units)\n",
    " .groupby(['month'])\n",
    " .agg({'max_temperature':['max','mean'], 'mean_temp':'mean'})\n",
    " .apply(lambda x: round(x, 2))\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cleaned_df\n",
    " .pipe(convert_units)\n",
    " .groupby(['station_number'])\n",
    " .agg({'max_temperature':['max','mean'], 'mean_temp':'mean'})\n",
    " .apply(lambda x: round(x, 2))\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We prefer predictive power score over correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps.predictors(cleaned_df.sample(10 ** 4), 'snow', sample=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~cleaned_df.snow).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the pps model_score value is not comparable with the accuacy of a naive model predicting always snow since it is an F1 score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The imbalance in the data could be a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(df, col):\n",
    "    return pd.concat([df[lambda df: df[col]],\n",
    "                         df[lambda df: ~df[col]].sample(df[col].sum())])\n",
    "balanced_df = balance(cleaned_df, 'snow')\n",
    "balanced_df.snow.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps.predictors(balanced_df, 'snow', sample=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df =pps.matrix(balanced_df.drop(columns=['fog', 'rain', 'hail', 'thunder', 'tornado']))[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore').round(2)\n",
    "sns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df = (pps.matrix(balanced_df[['snow', 'date', 'mean_temp', 'max_temperature', 'station_number', 'total_precipitation']])\n",
    "             [['x', 'y', 'ppscore']]\n",
    "             .pivot(columns='x', index='y', values='ppscore')\n",
    "             .round(2))\n",
    "sns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df[['snow', 'date', 'max_temperature', 'station_number', 'total_precipitation']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "If you made it up to here all by yourself, you can use your prepared dataset to train an Algorithm of your choice to forecast whether it will snow on the following date for each station in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "str(dt.datetime.today()- dt.timedelta(days=13*365)).split(' ')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are allowed to use any library you are comfortable with such as sklearn, tensorflow keras etc. \n",
    "If you did not manage to finish part one feel free to use the data provided in 'coding_challenge.csv' Note that this data does not represent a solution to Part 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "Again due to time constraints we decided to model the data as follows:\n",
    " - balance the training data\n",
    " - put together fixed length time series into data vectors\n",
    " - limited training data for each station prediction --> use one model predicting just snow or no snow from the data of the stations most suited for it\n",
    " - train a rbf-kernel SVC on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_multi_index(df):\n",
    "    df.columns = [\"_\".join(map(str,col)) for col in df.columns.values]\n",
    "    return df\n",
    "\n",
    "time_data = (pd.concat([(cleaned_df\n",
    "            .assign(date=lambda df:df.date+i*pd.Timedelta('1d'),\n",
    "                  date_shift=i\n",
    "                  )\n",
    "           ) for i in range(0,5)])\n",
    " .pivot(index='date', values=['max_temperature','total_precipitation', 'snow'],columns=['station_number','date_shift'])\n",
    " .pipe(join_multi_index)\n",
    " .dropna()\n",
    " .reset_index()\n",
    " .convert_dtypes()\n",
    ")\n",
    "time_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps.predictors(balance(time_data,'snow_1_0'), 'snow_1_0', sample=None)[lambda df: ~df.x.str.endswith('0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(i,j):\n",
    "    # how good does snow_i_0 predict snow_j_0\n",
    "    return pps.score(balance(time_data, f'snow_{j}_0'), f'snow_{i}_0', f'snow_{j}_0')['ppscore']\n",
    "\n",
    "matrix = np.array([[score(i,j) for j in range(1,11)] for i in range(1,11)])\n",
    "matrix_df = pd.DataFrame(matrix, index=pd.Series(range(1,11),name='y'), columns=pd.Series(range(1,11),name='x')).round(2)\n",
    "sns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_stations = [[i]+sorted(set(range(1,11))-{i,8}, key = lambda x:score(i,x), reverse=True)[:2] for i in range(1,11) if i != 8]\n",
    "best_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = best_stations[2]\n",
    "\n",
    "def new_names(stations):\n",
    "    return ({f\"snow_{stations[0]}_0\":'_y'}\n",
    "            |{\"date\":\"_date\"}\n",
    "            |{f\"{col}_{start}_{time}\":f\"_{col}_{target}_{time}\" \n",
    "                 for col in ['max_temperature','total_precipitation','snow']\n",
    "                 for target, start in tuple(enumerate(stations,1))\n",
    "                 for time in range(1,5)\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([(time_data\n",
    " .rename(columns=new_names(stations))\n",
    " .pipe(lambda df: df.drop(columns=[col for col in df.columns if col[0] != '_']))\n",
    " .rename(columns=lambda x:x.lstrip('_'))\n",
    ") for stations in best_stations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = set(map(lambda x:x.strip('_'+\"\".join(map(str,range(10)))), train_df.columns))-{'y'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_groups = [sorted([col for col in train_df.columns if col.startswith(group)]) for group in groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruff_model(train_df,col_groups,['y'], n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
